% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/is_decomp.R
\name{is_decomp}
\alias{is_decomp}
\alias{is_decomp_iso}
\alias{is_decomp_lin}
\title{Interval Score Decompositions}
\usage{
is_decomp(
  y,
  int,
  level = NULL,
  alpha1 = NULL,
  alpha2 = NULL,
  method = c("isotonic", "linear"),
  return_fit = F
)

is_decomp_iso(
  y,
  int,
  level = NULL,
  alpha1 = NULL,
  alpha2 = NULL,
  return_fit = F
)

is_decomp_lin(
  y,
  int,
  level = NULL,
  alpha1 = NULL,
  alpha2 = NULL,
  return_fit = F
)
}
\arguments{
\item{y}{vector of observations.}

\item{int}{matrix or dataframe containing the prediction intervals.}

\item{level}{nominal coverage level of the central prediction intervals.}

\item{alpha1, alpha2}{lower and upper quantile levels if evaluating non-central prediction intervals.}

\item{method}{method used to estimate the decomposition terms; one of 'linear' (default) and 'isotonic'.}

\item{return_fit}{logical specifying whether the recalibrated predictions used to estimate
the decomposition terms should be returned; default is \code{FALSE}.}
}
\value{
Vector containing the average interval score for the interval forecasts,
as well as the estimated uncertainty, discrimination, and miscalibration terms.

If \code{return_fit = TRUE}, a list is returned containing the interval score
decomposition terms and the recalibrated interval forecasts used to estimated
the decomposition terms.
}
\description{
Assess the conditional calibration of interval forecasts using decompositions of
the interval score.
}
\details{
\emph{Theory:}

Interval forecasts (or prediction intervals) are comprised of a lower bound \eqn{\ell}
and an upper bound \eqn{u}, with \eqn{\ell < u}. The forecast is made such that the
observation \eqn{y} is predicted to fall within the interval with a given coverage level \eqn{1 - \alpha}.
In the general case, it can be assumed that the prediction interval is \emph{non-central},
so that the probability that \eqn{y < \ell} is equal to \eqn{\alpha_1 \in (0, 1)} and the probability
that \eqn{y > u} is equal to \eqn{\alpha_2 \in (0, 1)}, with \eqn{\alpha_1 < \alpha_2}.
Typically, a \emph{central} prediction interval is issued, for which it is assumed that the
probability that \eqn{y < \ell} is equal to the probability that \eqn{y > u}, i.e.
\eqn{\alpha_1 = \alpha/2} and \eqn{\alpha_2 = 1 - \alpha/2}.

Competing interval forecasts can be compared using the interval score,
\deqn{\mathrm{IS}_{\alpha_1, \alpha_2}([\ell, u], y) = |u - y | + \frac{1}{\alpha_1} 1\{y < \ell\} (\ell - y) + \frac{1}{1 - \alpha_2} 1\{y > u\} (y - u)}.
In the case of central prediction intervals, the scaling factors \eqn{1 / \alpha_1} and \eqn{1 / (1 - \alpha_2)} both
simplify to \eqn{2 / \alpha}.

In practice, we observe several interval forecasts \eqn{[\ell_i, u_i]} and corresponding observations \eqn{y_i}
for \eqn{i = 1, \dots, n}, and we wish to compare forecasters or forecast methods based on
the average interval score
\deqn{\mathrm{\bar{S}} = \frac{1}{n} \sum_{i=1}^{n} \mathrm{IS}_{\alpha_1, \alpha_2}([\ell_i, u_i], y_i).}

While the average score \eqn{\mathrm{\bar{S}}} provides a single value that can be used to rank
different forecasters, it can also be additively decomposed into terms quantifying different
aspects of forecast performance. Most commonly, these terms measure how much \emph{uncertainty} there
is in the forecast problem, the forecast's \emph{discrimination} or information content, and
the degree of forecast \emph{miscalibration}. These terms can be calculated using the following decomposition:
\deqn{\mathrm{\bar{S}} = \mathrm{\bar{S}^R} - (\mathrm{\bar{S}^R} - \mathrm{\bar{S}^C}) + (\mathrm{\bar{S}} - \mathrm{\bar{S}^C}),}
where
\deqn{\mathrm{\bar{S}^R} = \frac{1}{n} \sum_{i=1}^{n} \mathrm{IS}_{\alpha_1, \alpha_2}(R, y_i)}
is the average interval score for a reference prediction interval \eqn{R},
and
\deqn{\mathrm{\bar{S}^C} = \frac{1}{n} \sum_{i=1}^{n} \mathrm{IS}_{\alpha_1, \alpha_2}(C_i, y_i)}
is the average interval score for \eqn{C_i}, which correspond to recalibrated versions of
the original prediction intervals \eqn{[\ell_i, u_i]}.

The first term of the decomposition, \eqn{\mathrm{\bar{S}^R}}, provides a baseline measure of forecast performance,
thereby quantifying the inherent uncertainty or unpredictability of the observations; the second term,
\eqn{\mathrm{\bar{S}^R} - \mathrm{\bar{S}^C}}, represents the improvement in accuracy that is obtained
from the recalibrated predictions compared to the (uninformative) reference predictions, thus providing a measure
of how much information is contained within the original interval forecasts; the third term,
\eqn{\mathrm{\bar{S}} - \mathrm{\bar{S}^C}}, is the improvement in accuracy that is obtained by recalibrating
the original interval forecasts, which quantifies the degree of miscalibration in the original predictions.
Note that the miscalibration corresponds to conditional calibration, where the conditioning is on the original
forecasts themselves.

This decomposition holds for any choice of the reference prediction \eqn{R} and recalibration method
to produce \eqn{C_i}. However, the interpretation of the terms requires specific choices. In particular,
it is desirable that the decomposition terms are non-negative; in this case, we can say that a miscalibration
of zero corresponds to a calibrated prediction interval, for example.

Two methods have been proposed in the literature to calculate the decomposition terms with
desirable theoretical guarantees. These correspond to recalibrating the original prediction intervals using
\emph{isotonic regression} and \emph{linear regression}. Details can be found in the references below.


\emph{Implementation:}

\code{y} should be a numeric vector of observations.

\code{int} should be a numeric matrix or dataframe containing the prediction intervals for \code{y}.
We should have that \code{nrow(int) = length(y)} and \code{ncol(int) = 2}. The first and
second columns of \code{int} should contain the lower and upper bounds of the prediction
intervals, respectively, so that \code{all(int[, 1] < int[, 2])}.

\code{level} should be the nominal coverage of the prediction intervals. For example, if \code{int}
contains central 90% prediction intervals for \code{y}, then \code{level = 0.9}.
This corresponds to \eqn{1 - \alpha} in the theoretical explanation above.
If the prediction intervals are non-central prediction intervals for \code{y}, then
\code{alpha1} and \code{alpha2} should be used instead of \code{level}; if both are used,
then \code{level} will be ignored.

\code{alpha1} and \code{alpha2} should be the lower and upper quantile levels corresponding
to the lower and upper bounds of \code{int}, if \code{int} contains non-central prediction
intervals for \code{y}. These correspond to \eqn{\alpha_1} and \eqn{\alpha_2} in the theoretical
explanation above. If the prediction intervals are central prediction intervals for \code{y}, then
it is simpler to use \code{level} instead of \code{alpha1} and \code{alpha2}.

\code{method} should be the method used to recalibrate the original prediction intervals to estimate
the interval score decomposition terms. This must be one of \code{'isotonic'} and \code{'linear'},
corresponding to isotonic and linear regression, respectively.

\code{return_fit} should be a logical specifying whether the recalibrated prediction intervals
should be returned alongside the decomposition terms. It may be useful to study the
recalibrated prediction intervals, in order to assess whether the assumptions made by
the recalibration method are appropriate, for example. The default is \code{return_fit = FALSE}.
}
\section{References}{


\emph{Linear decomposition:}

Dimitriadis, T., Puke, M., (2025).
`Statistical Inference for Score Decompositions under Linear Recalibration'

\emph{Isotonic decomposition:}

Allen, S., Burnello, J. and Ziegel, J. (2025):
`Assessing the conditional calibration of interval forecasts using decompositions of the interval score'.
\emph{arXiv pre-print}.
}

\examples{

n <- 1000 # sample size
alpha <- 0.1 # 90\% prediction intervals
mu <- rnorm(n)
y <- rnorm(n, mean = mu, sd = 1) # simulate observations

# Ideal forecaster: F = N(mu, 1)
L_id <- qnorm(alpha/2, mu)
U_id <- qnorm(1 - alpha/2, mu)
int_id <- data.frame(Lower = L_id, Upper = U_id)

# isotonic decomposition
out_iso <- is_decomp(y, int_id, level = 1 - alpha) # using level
out_iso2 <- is_decomp(y, int_id, alpha1 = alpha/2, alpha2 = 1 - alpha/2) # using alpha1 and alpha2
out_iso3 <- is_decomp_iso(y, int_id, level = 1 - alpha) # using is_decomp_iso
all.equal(out_iso, out_iso2)
all.equal(out_iso, out_iso3)

# linear decomposition
out_lin <- is_decomp(y, int_id, level = 1 - alpha, method = "linear") # using level
out_lin2 <- is_decomp(y, int_id, alpha1 = alpha/2, alpha2 = 1 - alpha/2, method = "linear") # using alpha1 and alpha2
out_lin3 <- is_decomp_lin(y, int_id, level = 1 - alpha) # using is_decomp_lin
all.equal(out_lin, out_lin2)
all.equal(out_lin, out_lin3)

# non-central prediction intervals

alpha1 <- 0.05
alpha2 <- 0.9

# Ideal forecaster: F = N(mu, 1)
L_id <- qnorm(alpha1, mu)
U_id <- qnorm(alpha2, mu)
int_id <- data.frame(Lower = L_id, Upper = U_id)

out_iso <- is_decomp(y, int_id, alpha1 = alpha1, alpha2 = alpha2) # isotonic
out_lin <- is_decomp(y, int_id, alpha1 = alpha1, alpha2 = alpha2, method = "linear") # linear


}
\seealso{
\code{\link{interval_score}} \code{\link{coverage}} \code{\link{count_comparables}} \code{\link{plot_mcbdsc}}
}
\author{
Sam Allen
}
